p8105_hw2_lz3015
================
2025-09-30

## Loading packages

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(dplyr)
```

## Problem 1

cleaning pols-month

``` r
pols_df = 
  read.csv(
    "./fivethirtyeight_data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    month = month.name[month], 
    president= if_else(prez_gop == 1, "gop", "dem")
  ) |> 
  select(-prez_dem, -prez_gop, -day) #drops these variables
```

cleaning snp

``` r
snp_df = 
  read.csv(
    "./fivethirtyeight_data/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(year),
    year = if_else(year > 20, 1900 + year, 2000 + year),
    month = as.integer(month),
    month = month.name[month], 
  ) |> 
  select(-day) |> 
  arrange(year)
```

cleaning and tidying unemployment

``` r
unemployment_df = 
    read.csv(
      "./fivethirtyeight_data/unemployment.csv") |>
  rename(year = Year) |> # removing capital y for consistency
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment") |> 
  mutate(
    month = match(month, month.abb),
    month = month.name[month]
  )
```

Joining the three datasets

``` r
# join with unemployment data
join_df = pols_df |>
  left_join(snp_df, by = c("year", "month")) |>
  left_join(unemployment_df, by = c("year", "month"))
```

## Data description:

The `pols_df` data contains 822 observations related to the number of
national politicians who are democratic or republican at any given time.
This data set contains information as early as January 1947 and goes all
the way to June of 2015 and there are 9 variables

The `snp_df` data contains 787 observations related to Standard & Poor’s
stock market index (S&P) represented by the variable “close” (the
closing values of the S&P stock index on the associated date),often used
as a representative measure of stock market as a whole. This data was
also tracked over time starting in February of 2001 and ending in 2012
and there are 3 variables

The `unemployment_df` data contains 816 observations detailing the
unemployment rate between January 1948 and December 2015 and there are 3
variables

The `join_df` data is the combination of the three data sets described
above with 822 observations and 11 variables

## Problem 2

``` r
library(readxl)
```

Mr. Trash Wheel

``` r
mr_trash_df = read_excel("trashwheel_data/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel",
    skip = 1) |> 
  janitor::clean_names() |> 
  select(-starts_with("x")) |>  # drops empty columns
  filter(!is.na(dumpster)) |>
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr. Trash Wheel",
    year = as.integer(year) # Need to standardize data type for year for future joins
  )
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
prof_trash_df = read_excel("trashwheel_data/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    skip = 1) |> 
  janitor::clean_names() |> 
  select(-starts_with("x")) |>  # drops empty columns
  filter(!is.na(dumpster)) |>
  mutate(
    trash_wheel = "Professor Trash Wheel",
    year = as.integer(year) # Need to standardize data type for year for future joins
  )
```

``` r
gwynnda_trash_df = read_excel("trashwheel_data/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynns Falls Trash Wheel",
    skip = 1) |> 
  janitor::clean_names() |> 
  select(-starts_with("x")) |>  # drops empty columns
  filter(!is.na(dumpster)) |>
  mutate(
    trash_wheel = "Gwynns Falls Trash Wheel" ,
    year = as.integer(year) # Need to standardize data type for year for future joins
  )
```

``` r
all_trash_df = 
  bind_rows(
    mr_trash_df,
    prof_trash_df,
    gwynnda_trash_df
)
```

The `mr_trash_df` data contains 707 observations and 15 variables. The
`prof_trash_df` data contains 132 observations and 14 variables. The
`gwynnda_trash_df` data contains 349 observations and 13 variables.
`weight_tons` and `volume_cubic_yards` are among the main descriptive
variables that illustrate the magnitude of the trash collected. The
average weight collected on a given day in tons for each wheel is:
Mr. Trash Wheel = 3.2; Professor Trash Wheel = 2.1; Gwynnda Trash Wheel
= 3.1; The overall average `weight_tons` from the combined
`all_trash_df` is 3.1 The recorded data spans from 2014 to 2025

## Problem 3

``` r
zip_df = 
  read.csv (
  "./zillow_data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  rename(
    borough = county
  )

#Renaming borough names 
zip_df = zip_df |> 
  mutate(
    borough = recode(
    borough,
    "Richmond" = "Staten Island",
    "New York" = "Manhattan",
    "Kings" = "Brooklyn"
))
```

View df to make sure columns have proper labels and boroughs were
recoded

``` r
view(zip_df)
```

After looking at view, we should check for zip code duplicates

``` r
zip_df |> 
  group_by(zip_code) |> 
  filter(n() > 1) |> 
  ungroup()
```

    ## # A tibble: 4 × 7
    ##   borough   state_fips county_code county_fips zip_code file_date neighborhood  
    ##   <chr>          <int>       <int>       <int>    <int> <chr>     <chr>         
    ## 1 Bronx             36           5       36005    10463 7/25/07   Kingsbridge a…
    ## 2 Brooklyn          36          47       36047    11201 7/25/07   Northwest Bro…
    ## 3 Manhattan         36          61       36061    10463 7/25/07   Kingsbridge a…
    ## 4 Manhattan         36          61       36061    11201 7/25/07   Northwest Bro…

Remove duplicates:

``` r
zip_df_unique = zip_df |> 
  distinct(zip_code, .keep_all = TRUE)
view(zip_df_unique)
```

\##clean repeat Zips all dates start with X so filter

``` r
zori_df =
  read.csv(
    "./zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |>
  janitor::clean_names() 
  view(zori_df)
```

``` r
zori_tidy = zori_df |> 
  # Pivot to long format
  pivot_longer(
    cols = starts_with("x"),
    names_to = "date",
    values_to = "zori") |> 
  mutate(
    date = str_remove(date, "x"),
  ) |> 
    separate(date, into = c("year", "month", "day"), sep = "_") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    month = month.name[month]) |>
  mutate(
    county_name = recode(
      county_name,
    "Richmond County" = "Staten Island",
    "New York County" = "Manhattan",
    "Kings County" = "Brooklyn",
    "Queens County" = "Queens",
    "Bronx County" = "Bronx")) |> 
  rename(borough = county_name, zip_code = region_name, zori_year = year, zori_month = month) |>
  select(-day)
```

view df to check that pivot worked accordingly

``` r
view(zori_tidy)
```

join zori dataframe with zip dataframe on zip_code

``` r
zori_zip_join = zori_tidy|> 
  left_join(zip_df_unique, by = "zip_code") |> 
  select ( metro, borough.x, neighborhood, zip_code, zori_year, zori_month, zori,)

view(zori_zip_join)
```

# Joined data descriptors

``` r
total_observations = nrow(zori_zip_join)
print(paste("Total observations:", total_observations))
```

    ## [1] "Total observations: 17284"

``` r
unique_zips = n_distinct(zori_zip_join$zip_code)
print(paste("Unique ZIP codes:", unique_zips))
```

    ## [1] "Unique ZIP codes: 149"

``` r
unique_neighborhoods = n_distinct(zori_zip_join$neighborhood)
print(paste("Unique neighborhoods:", unique_neighborhoods))
```

    ## [1] "Unique neighborhoods: 43"

In the joined dataset, there are 17284 total observations, 149 unique
zip codes and 43 unique neighborhoods

# Find the ZIP codes that are in zip_df_unique but not in zori_tidy

``` r
missing_zips = zip_df_unique |>
  anti_join(zori_tidy, by = "zip_code")
view(missing_zips)
num_missing_zips <- nrow(missing_zips)
print(num_missing_zips)
```

    ## [1] 171

There were 171 zip codes that were in the zip code dataset but not in
the zori data set. This could be due to many reasons such as heavy
commercial/industrial or private use within these zip codes that prevent
rental activity. For example, after a quick google search the zip code
10464 was City Island in the Bronx that is a very small island with what
appears to be beaches, parks and small shops. It is very likely that the
rental activity is very low here which is why this zip code does not
have a zori score.

# Covid comparison

``` r
price_comparison_table = zori_zip_join |> 
  # Filter for January 2020 and January 2021 data
  filter(zori_year %in% c(2020, 2021), zori_month == "January") |> 
  select(zip_code, borough.x, neighborhood, zori, zori_year) |> 
  # Pivot to a wider format to compare prices
  pivot_wider(
    names_from = zori_year,
    values_from = zori,
    names_prefix = "zori_"
  ) |> 
  # Calculate the change in price
  mutate(zori_change = round(zori_2021 - zori_2020, digits = 0)) |> 
  arrange(zori_change) |> 
  # Select the top 10 rows
   slice_head(n = 10) |>
  select(zip_code, borough.x, neighborhood, zori_change)
  print(price_comparison_table)
```

    ## # A tibble: 10 × 4
    ##    zip_code borough.x neighborhood                  zori_change
    ##       <int> <chr>     <chr>                               <dbl>
    ##  1    10007 Manhattan Lower Manhattan                      -913
    ##  2    10069 Manhattan <NA>                                 -748
    ##  3    10009 Manhattan Lower East Side                      -714
    ##  4    10016 Manhattan Gramercy Park and Murray Hill        -712
    ##  5    10002 Manhattan Lower East Side                      -710
    ##  6    10001 Manhattan Chelsea and Clinton                  -710
    ##  7    10004 Manhattan Lower Manhattan                      -706
    ##  8    10038 Manhattan Lower Manhattan                      -698
    ##  9    10012 Manhattan Greenwich Village and Soho           -686
    ## 10    10010 Manhattan Gramercy Park and Murray Hill        -685

All of the neighborhoods with the largest drop in ziro scores were
located in Manhattan which was hit particularly hard by the COVID-19
pandemic. COVID-19 spreads more rapidly in high population density
environments which likely drove fear into the Manhattan rental market
causing rental prices to drop dramatically during the pandemic.
